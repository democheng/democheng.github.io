---
layout:     post
title:      不用任何公式告诉你什么是增强现实(AR)
subtitle:   naive tutorial to argumented reality
date:       2018-10-26
author:     democheng
header-img: img/post-bg-ioses.jpg
catalog: true
tags:
    - AR
    - SLAM
    - 三维重建
    - SFM
---

## 目标

不用任何公式告诉你什么是增强现实(AR)

---

![ar1](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar1.png)

---

## 目录
本次（不严谨的）科普大约分为四个部分：
- AR一瞥：展示一个AR样例，然后简单地对其进行技术讲解，提出该样例需要解决的问题；
- 庖丁解牛：将这些问题数学模型化，模块化；
- 图文并茂：看图说话，保证不涉及任何公式；
- 参考文献：除了参考文献，还有笔者在自学过程中，发现的有价值的，容易理解的文档，课程；

本文使用的图示，基本都是笔者自己画的，**使用图示的时候写明出处**即可；

---

![ar2](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar2.png)

---

demo视频如下，点击进入youtube（随意找的AR demo视频，无任何利益相关）：

---

[![ardemo](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar_demo.png)](https://www.youtube.com/watch?v=_WRul7mvnqY)

---

## AR一瞥
- 在demo的视频序列帧中，假设我们在连续帧之间找到了一些对应的特征点（后续详细解释），我们通过某种计算（后续详细解释）得到这些特征点的三维空间位置和相机在坐标系中的**位置和朝向**（这里用**位姿**简称），从而完成**真实尺度**的三维重建；

- 假设我们事先做好了1:1的沙发模型，在我们重建好的世界里，加载这个沙发模型；

- 接着，我们继续计算每一帧图像的相机**位姿**；

- 最后，对带有三维模型的世界进行相机映射（重新拍照），就得到demo中的AR效果；


---

![ar3](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar3.png)

---

---

![ar4](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar4.png)

---

## 庖丁解牛

我们这里简单地把整个AR算法分成4个模块，其中，特征模块本身只是挑选了稀疏特征（sparse feature）这一类算法进行讲解，semi-dense feature based，dense feature based暂不介绍：
- 特征模型
- 匹配（跟踪）模型
- 几何模型
- 优化（残差）模型

---

![ar5](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar5.png)

---

### 特征模型
直白地来讲，**特征**就是对一个对象的描述，如图所示，骆驼和羊驼用同样的6个标准来进行描述，就是：

---

![ar6](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar6.png)

---

当我们把这些描述用一个**向量**表示，就有点数学模型的样子了，关于 **特征向量（feature vector）** 和 **特征描述子（feature descriptor）** 的不严谨定义如下页PPT所示，这里也简单地罗列了一下特征描述子的发展史，有兴趣的同学可以了解一下；在 **深度学习(DL)** 雄霸天下之前，都是用传统 **机器学习（ML）** 加**特征描述子**来做目标检测,分类等任务的，传统的**特征描述子**是人为设定的算法（规则）进行计算的，非常依赖设计者的水平， **深度学习(DL)** 则是自动地学习特征，是广大小白的福音．

---

![ar7](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar7.png)

---

### 匹配模型
直白地来讲，匹配模型就是计算**特征向量**之间的相似性或者差异性，对差异性小于某个阈值的匹配，我们就认为他们匹配上了；
而跟踪模型，本质上来讲也是匹配模型，只是我们使用了运动先验，比如，我之前一步走了1m，那么根据匀速运动模型，我下一步也很有可能走了1m，寻找匹配对的时候，就在1m附近找就好了；有了运动先验，可以让匹配模型更快更 **鲁棒** (robust,形容算法很NB,很稳定的意思)．

---

![ar8](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar8.png)

---

---

![ar9](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar9.png)

---

## 几何模型
看完**特征向量**和**特征匹配**之后，我们来介绍一下**射影几何**，有兴趣深入研究的同学可以认真看看[Multiple View Geometry in Computer Vision, Second Edition](http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20(Second%20Edition).pdf)；这里大家只要看看不同的变换对几何性质的影响，有个直观的认识即可：

---

![ar10](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar10.png)

---

简单来看相机模型，显示物体上的点与光心的连线与像平面的交点进行成像，这个图像就是对物体进行**射影变换**得到的；我们的目标是对图像中的场景进行**三维重建**，仅仅用单目相机，在不知道任何相机信息（**内参**）的情况下，我们只能进行**摄影重建**；在知道**内参**的情况下，我们最多能进行**相似重建**，**尺度**是不可能知道的；**尺度**的估计需要加入额外信息，我们会在后面解释原因，并给出解决方案；

---

![ar11](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar11.png)

---

---

![ar12](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar12.png)

---

---

![ar13](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar13.png)

---

为了让图示更清晰易懂，我们使用二维图示进行说明：

---

![ar14](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar14.png)

---

即便我们有一序列图像，也无法得到现实目标的**真实尺度**，对应的，相机的真实**位置**也是歧义的：

---

![ar15](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar15.png)

---

但是，当我们知道两帧图像之间的 **刚体变换** (即**欧式变换**)时候，我们就可以消除歧义，得到现实目标的**真实尺度**，同时得到相机的真实**位置**；

---

![ar16](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar16.png)

---

那么，实际项目中如何解决这个问题呢？我们来看看平常用的手机：

---

![ar17](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar17.png)

---

这里，请注意，我们是假设IMU完全没有误差来进行解释的，但是实际是IMU有噪声，价格越贵，尺寸越小，噪声越小；图像信息和IMU的信息是互补的，IMU给图像提供**粗糙的尺度**，图像给IMU提供信息帮助矫正其噪声，正确的**初始化**(估计IMU的噪声)可以使得visual-inertial(visual代表视觉信息，即图像，inertial代表惯性信息，即IMU)系统保持稳定；

---

![ar18](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar18.png)

---

## 优化（残差）模型
理想中的几何模型如图所示，红色的三维点假设是已知的，并且映射到图像上没有任何噪声干扰；虚线是假想的光线，这些假想的虚线对像素点与其对应的三维点构成了几何约束，这个几何约束就是**对极几何**　，也是我们求解三维点的**位置**　（坐标）的理论基础；这里我们只是图示一下，不做数学上的介绍；

---

![ar19](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar19.png)

---

回到问题本身，红色的三维点的**位置**我们事先并不知道，并且映射过程是有噪声的，因此图像上的像素点也是不准确的；
我们可以通过之前提到的**对极几何**，在知道相机**内参**的情况下，线性求解三维点的**位置**以及相机没有尺度的**位姿** (为什么没有尺度，请回顾前几页PPT)；这个过程可以用一般SFM的流程线性求解，有兴趣可以google之，这里不详述；

再次强调一遍，这里求解输出的是：
- 三维点的**位置**
- 相机没有尺度的**位姿**

---

![ar20](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar20.png)

---

然后再线性解的基础上，进行**非线性优化** （提高求解精度）；通过下面两页PPT可以知道求解的三维点的**位置**与真实的三维点越来越接近；

**注意** ：对真实图像数据而言，真实的三维点的**位置**永远不可能知道，因为测量总是有噪声的，我们只能无限逼近它；

---

![ar21](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar21.png)

---

**非线性优化**是数据建模中经常提到的名词，无论是本文讲的**增强现实**，还是如日中天的**深度学习**，都跟**非线性优化**有关；我们这里用一个**非线性优化函数**的图示来大约解释下**非线性优化**到底是什么；

对于**非线性优化**，我们只需要了解：
- **优化函数**包括**优化对象**和**优化目标**；
- **优化对象**是什么；
- **优化目标**是什么；

如图所示，**优化对象**的变化会导致**优化目标**的变化，不同的**优化对象**对应不同的**海拔**，我们的目标是找到一个**峡谷**，黑色的轨迹就是某个优化的过程，不同的起点可能到达不同的终点，得到的结果不一定保证是全局最优，也就是所谓的**局部最小**（掉进山谷爬不起来了）；

我们的**几何模型**：
- **优化对象**是检测到的图像**特征点**所对应的三维点的**位置**和所有的相机没有尺度的**位姿**；
- **优化目标**是求解得到的图像**特征点**所对应的三维点反投影回图像的**像素残差**最小；

**注意**：

- 图示表示的函数并不是我们这里提到的**几何模型**，可以理解成它的简化版本，只是为了解释；
- 如果我们加入了**基线**（无论是双目相机得到还是借助IMU得到）约束，得到的就是所有的相机**真实尺度**的**位姿**；

---

![ar22](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar22.png)

---

如果我们把两个相机的约束，推广到N个相机的约束，并且用同样的方法建立**优化函数**，就是我们常说的**BA**，字面意思就是在**光**线约**束**条件下，通过优化，**平**均误**差**；如果我们加入了**基线**（无论是双目相机得到还是借助IMU得到）约束，得到的就是所有的相机**真实尺度**的**位姿**；

---

![ar23](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar23.png)

---

可能到这里大家对**三维点**，**相机**，**位姿**等的概念还有些许模糊，我们再来看一段TUM2018年ICRA的visual-inertial-direct-sparse-odometry的demo视频，帮助大家更形象地理解：

---

[![vi_dso](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar24.png)](https://www.youtube.com/watch?v=GoqnXDS7jbA)

---

好的，到这里，我们基本上把原理上的东西过了一遍，现在总结回顾一下AR的流程，看看重点要解决的几个问题：
- **优化对象**是检测到的图像**特征点**所对应的三维点的**位置**和所有的相机没有尺度的**位姿**；
- **三维重建**了现实世界后，放入米老鼠，再用当前相机**映射**（就是拍照的意思）；
- 完成与米老鼠的AR合影；

---

![ar25](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar25.png)

---

我们再简单提一下相关的术语，并给他们一个范畴划分：

---

![ar26](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar26.png)

---

---

![ar27](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar27.png)

---

---

![ar28](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar28.png)

---

## 参考文献

---

![ar29](https://github.com/democheng/democheng.github.io/raw/master/img/ar/ar29.png)

---


